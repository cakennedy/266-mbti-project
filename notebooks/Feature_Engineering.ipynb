{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cakennedy/266-mbti-project/blob/main/notebooks/Feature_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEtLK_JunA3i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering Ideas\n",
        "\n",
        "**From Reddit: A Gold Mine for Personality Predicetion**\n",
        "\n",
        "Linguistic Features: They used LIWC (Pennebaker at al., 2015), a widely used NLP tool in personality prediction to extract 93 features. This includes standard word count statistics(word count, words longer than six letters, number of prepositions, etc), psychological processes(emotional, cognitive, sensory and social processes), relativity(words about time, the past, the future), personal concerns(such as occupation, financial issues, health) and other dimensions such as counts of various punctionation and swear words. Complimentary to LIWC, they also used a number of psycholinguistic word lists including perceived happiness, affective norms (e.g. valence, arousal, and dominance), imageability, and sensory experience, described in Preotiuc-Pietro et al. (2017) as well as two lists of word meaningfulness ratings from the MRC Psycholinguistic Database (Coltheart, 1981). For each user, they calculated the average ratings for every word count from these dictionaries that gave them 26 features, denotes PSYCH. \n",
        "\n",
        "User Activity Features: They used number of posts. They also used topical affinity features, computing comment counts for the user across subreddits and encoded those as a single vector, together with the entropy of the corresponding distribution (?). They also included time intervals between comment timestamps (the mean, median and maximum delay) as well as daily, weekly and monthly distribution of posts. \n",
        "\n",
        "There is a pretty nice table (#4) on page 92 of this paper that shows each feature group and maps it to how effective that group was at predicting each of the four letters individually. It is roughly sorted from most effective features to least effective overall. The top three were \"global\", \"psy\" - presumably PSHCY, and LIWC. Interestingly, day of the week seemed to be highly predictive of the J/P dimension. Month of posts seemed to be indicative of N vs I. Other comments, \"sensing types are more concrete while intuitives are more abstract which seems to be reflected in the imageability feature. Intuitives use more rare, more complex and longer words. Sensing types also tend to share posts with content they found outside more than intuitives.\"\n",
        "\n",
        "They said, \"In line with standard practice, we frame the MBTI personality prediction task as four independent binary classification problems. \"\n",
        "\n",
        "**Concatenating Posts - analyze users instead of posts** \n",
        "\n",
        "Celli and Lepri 2018 and Stajner and Yenikent 2020 and 2021 came to the conclusion that tweets might not contain sufficient amounts of MBTI signals (even after concatenating up to 150 - 200 tweets per user) due to the nature of Twitter posts. \n",
        "\n",
        "Sanja Stajner and Serek Yenikent used this approach in, *How to obtain Reliable Labels for MBTI Classification from Texts*. They used 2000 twitter posts per user in their classifier. One interesting comment from that study referring to prior work,\"In most of the languages, the best classifers outperformed the majority-class baselines for only the E/I and T/F dimensions\"\n",
        "\n",
        "The point here is one idea MIGHT be to try the post concatenation idea, which is how I did it with the social media personality test on my site. I recall for instance putting in 35,000 words vs. 100,000 words and finding that the more text I provided to it, the more accurate it was. This is an example of what the test result looked like https://www.typologycentral.com/threads/social-media-personality-test.80673/page-13#post-3345720.  Unfortunately it is no longer functional. I did find that the forum posts and Tumblr worked pretty well. Twitter and Facebook posts performed poorly. \n",
        "\n",
        "**From Text Analytics: What does your LinkedIn profiie summary say about your personality:**\n",
        "\n",
        "This paper had some really good ideas. \n",
        "Linguistic Features: \"Studies by (Maitresse et al., 2006; Mehl et al, 2006) note that extoversion is marked by greater use of verbs, adverbs and pronouns. Sentences of extroverted types also tend to be simpler with fewer words, lower lexical diversity and fewer negations. Conversely, introverts generally use more negating words and use a wider vocabulary. To identify these difference, a formality score (Heylighen and Dewaele, 1999) was calculated for each user post: \n",
        "\n",
        "Formality = 1/2(noun frequency + adjective frequency + preposition frequency + article frequency) - 1/2(pronoun frequency + verb freqiuency + adverb frequency + interjection frequency) + 50\"\n",
        "\n",
        "They used features to capture lexical diversity and word usage\n",
        "1. Type to token ratio: the number of unique words divided by the total number of words\n",
        "2. Average words per sentence: a proxy for sentence complexity\n",
        "3. Average word length: a proxy for vocabulary use\n",
        "\n",
        "They also used Psycholinguistic features. The National Research Council emotion lexicon was used to identify words along e basic poles:\n",
        "* joy-sadness\n",
        "* anger-fear\n",
        "* trust-disgust\n",
        "* anticipation-suprise\n",
        "The lexicon was used to determine the intensity of words along valence, arousal and dominance dimensions as well as two additional sentiment score3s (positive and negative) for approximately 20,000 words. This gave a total of 13 scores per word. Each of the 13 NRC dimensions were converted to features by weighting the value of the individual NRC word values by their frequency in each user post and representing each feature as a fraction of total words written in each post. That produced 13 NRC feature fectors for each post that were concatenated onto the tf-idf matrix for classifier training. \n",
        "\n",
        "They had an excellent table on page 6 of the paper that describes each feature in detail. \n",
        "\n",
        "**Ideas For Optimizing Transformer Architecture**\n",
        "\n",
        "There was one paper, Predicting Myers-briggs Type Indicator with Text Classification by Rane Hernandez and Ian Scott Knight. They used transformers. The paper is short and it would be good to look at it. A few summary points:\n",
        "* They tried both analyzing posts and analzing users. Analyzing individaul posts did not perform well. User classification (using a corpus of posts from a user instead of individual posts) performed much better. I didn't really understand or comprehend this point when I first read the paper.\n",
        "* They removed linkds to websites and stop words. \n",
        "* They used lemmatization\n",
        "* Using a Keras word tokenizer, they tokenized the 2500 most common words of the lemmatized text and other words were removed\n",
        "* They analyzed posts, not users and used padding same as us\n",
        "* GloVe was used to create the embeddings\n",
        "* They tried a number of RNNs including SimpleRNN, GRU, LSTM and Bidrectional LSTM. LSTM gave them the best results\n",
        "* They used Adam and binary crossentropy for the loss function\n",
        "* They used the four different binary classifiers\n",
        "* The best performance in testing came from batch size of 128, token vocabulary size of 2500, input vector length of 40, embedding vector length of 50, and 30 epochs (we wouldn't need that many for BERT though)\n",
        "\n",
        "Importantly, to analyze users (vs posts), they took the mean of the class probability for all of the posts in a user's corpus and rouded it to either 0 or 1 to predict the letter type. \n",
        "\n",
        "**Machine Lerning Approach to Personality Type Prediction Based on the Myers-Briggs Type Indicator**\n",
        "\n",
        "They built on the work that Hernandez and Knight did with transformers and used the same dataset and preprocessing. \n",
        "The main contributions of this paper seem to be:\n",
        "* They explained how the data is imbalanced on these personality type forums but their explanation of how they corrected for this didn't make any sense.\n",
        "* They looked at all different research including recent transformer work and decided to use a basic token count embedding approach combined with something called Gradient Boosting (XGBoost in sklearn), which peformed considerably better than the Hernandez and Knight RNN model (page 13 table 5). \n",
        "* As with Hernadez and Knight, they said very little about the features they used\n",
        "\n",
        "They used lemmatization and removed stop words. They also used the 4 way binary classification approach.\n",
        "\n",
        "**Personality Classification from Online Text using Machine Learning Approach**\n",
        "\n",
        "Khan, Ahmad, Asghar, Saddoza, Arif, and Khalid\n",
        "* They highlighted that the skewness of the dataset is the main issue with prior work which they addressed by oversampling (resampling)\n",
        "* They had an excellent analysis and table of prior work done starting on page 461\n",
        "* They used XGBoost and claimed to get a 99% precision and accuracy\n",
        "\n",
        "#Summary of Ideas\n",
        "\n",
        "The ideal option would be to shift to an analysis of users instead of posts. One paper describes analyzing a corpus of posts for users. Others imply combining post data (e.g., x number of posts or x words). \n",
        "\n",
        "Sex is an important feature if available. It is a strong predictor of T vs F. I believe age and number of posts will be useful features for a number of reasons.  \n",
        "\n",
        "I liked the feature set in the Reddit Gold Mine paper - specifically the LIWC and PSYCH features. The PSYCH features seem to align with what is in the LinkedIn paper that discusses the NRC emotion lexicon. I also liked the formality score \n",
        "\n",
        "***I purchased a 90 day academic license for LIWC. The license number is LIWC22-155B9757-5ECA4AE9-82F82D75-942BE0B7***\n",
        "\n"
      ],
      "metadata": {
        "id": "pXz-KVlRnFt9"
      }
    }
  ]
}